{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565c5c1e-8973-4045-83ec-cedd78710a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import shap\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9533ab-89cf-4628-b652-9dfaf36220df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED MODEL BUILDING AND EVALUATION\n",
      "================================================================================\n",
      "Loading pre-trained models and data...\n",
      "Error loading saved models: [Errno 2] No such file or directory: 'best_export_model.pkl'\n",
      "Please run Notebook 03 first to train models\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_export_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pre-trained models and data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     best_export_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_export_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     best_class_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_classification_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     export_scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_export_model.pkl'"
     ]
    }
   ],
   "source": [
    "# 1. SETUP AND DATA LOADING  \n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED MODEL BUILDING AND EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load pre-trained models and data\n",
    "print(\"Loading pre-trained models and data...\")\n",
    "\n",
    "try:\n",
    "    best_export_model = joblib.load('best_export_model.pkl')\n",
    "    best_class_model = joblib.load('best_classification_model.pkl')\n",
    "    export_scaler = joblib.load('export_scaler.pkl')\n",
    "    class_scaler = joblib.load('classification_scaler.pkl')\n",
    "    \n",
    "    # Load training data\n",
    "    df = pd.read_csv('world_trade_cleaned.csv')\n",
    "    country_df = df[df['Is_Country']].copy()\n",
    "    \n",
    "    # Load feature lists\n",
    "    with open('export_features.txt', 'r') as f:\n",
    "        export_features = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('classification_features.txt', 'r') as f:\n",
    "        class_features = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(\"✓ Models and data loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading saved models: {e}\")\n",
    "    print(\"Please run Notebook 03 first to train models\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba6d58-36eb-40ea-9618-b69a7196b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HYPERPARAMETER OPTIMIZATION \n",
    "\n",
    "print(\"\\n2. HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "print(\"Performing advanced hyperparameter optimization...\")\n",
    "\n",
    "# Prepare data (using subset for faster optimization)\n",
    "X_train_export = pd.read_csv('world_trade_countries_only.csv')\n",
    "# Filter to get features needed for export prediction\n",
    "X_train_export = X_train_export[export_features].dropna()\n",
    "y_train_export = X_train_export['Export (US$ Thousand)_imputed'] if 'Export (US$ Thousand)_imputed' in X_train_export.columns else None\n",
    "\n",
    "if len(X_train_export) > 1000:\n",
    "    X_train_sample = X_train_export.sample(1000, random_state=42)\n",
    "    if y_train_export is not None:\n",
    "        y_train_sample = y_train_export.loc[X_train_sample.index]\n",
    "    else:\n",
    "        y_train_sample = None\n",
    "else:\n",
    "    X_train_sample = X_train_export\n",
    "    y_train_sample = y_train_export\n",
    "\n",
    "if y_train_sample is not None and len(X_train_sample) > 100:\n",
    "    # Scale features\n",
    "    X_scaled_sample = export_scaler.transform(X_train_sample)\n",
    "    \n",
    "    # Define Optuna objective function\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "        }\n",
    "        \n",
    "        model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Use cross-validation\n",
    "        scores = cross_val_score(model, X_scaled_sample, y_train_sample, \n",
    "                                cv=3, scoring='r2', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Running Bayesian optimization with Optuna...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters found:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best CV R²: {study.best_value:.4f}\")\n",
    "    \n",
    "    # Train optimized model\n",
    "    best_params = study.best_params\n",
    "    optimized_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "    optimized_model.fit(X_scaled_sample, y_train_sample)\n",
    "    \n",
    "    # Save optimized model\n",
    "    joblib.dump(optimized_model, 'optimized_export_model.pkl')\n",
    "    print(\"✓ Optimized model saved\")\n",
    "else:\n",
    "    print(\"Insufficient data for hyperparameter optimization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ba6d26-76c2-42ce-b42e-7875a990dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MODEL COMPARISON WITH STATISTICAL TESTS\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_export_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load or create multiple model predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m models_to_compare \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbest_export_model\u001b[49m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_export_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(best_export_model)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: XGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare test data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_data \u001b[38;5;241m=\u001b[39m country_df[export_features \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExport (US$ Thousand)_imputed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear_Value\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_export_model' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. MODEL COMPARISON WITH STATISTICAL TESTS \n",
    "\n",
    "\n",
    "print(\"\\n3. MODEL COMPARISON WITH STATISTICAL TESTS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Load or create multiple model predictions\n",
    "models_to_compare = {\n",
    "    'Random Forest': best_export_model,\n",
    "    'Linear Regression': joblib.load('best_export_model.pkl') if 'linear' in str(best_export_model).lower() else None,\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Prepare test data\n",
    "test_data = country_df[export_features + ['Export (US$ Thousand)_imputed', 'Year_Value']].dropna()\n",
    "test_data = test_data[test_data['Year_Value'] == test_data['Year_Value'].max()]\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    X_test = test_data[export_features]\n",
    "    y_test = test_data['Export (US$ Thousand)_imputed']\n",
    "    X_test_scaled = export_scaler.transform(X_test)\n",
    "    \n",
    "    # Collect predictions\n",
    "    predictions = {}\n",
    "    for name, model in models_to_compare.items():\n",
    "        if model is not None:\n",
    "            try:\n",
    "                if name != 'Random Forest':\n",
    "                    model.fit(X_test_scaled[:100], y_test[:100])  # Quick fit\n",
    "                preds = model.predict(X_test_scaled)\n",
    "                predictions[name] = preds\n",
    "                print(f\"{name:20} R²: {r2_score(y_test, preds):.4f}\")\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Perform Diebold-Mariano test for model comparison\n",
    "    if len(predictments) >= 2:\n",
    "        print(\"\\nPerforming model comparison tests...\")\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = {}\n",
    "        for name, preds in predictions.items():\n",
    "            errors[name] = y_test - preds\n",
    "        \n",
    "        # Compare each pair\n",
    "        model_names = list(errors.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                m1, m2 = model_names[i], model_names[j]\n",
    "                \n",
    "                # Calculate difference in squared errors\n",
    "                diff = errors[m1]**2 - errors[m2]**2\n",
    "                \n",
    "                # Perform t-test\n",
    "                t_stat, p_value = stats.ttest_1samp(diff, 0)\n",
    "                \n",
    "                print(f\"\\n{m1} vs {m2}:\")\n",
    "                print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "                print(f\"  p-value: {p_value:.4f}\")\n",
    "                if p_value < 0.05:\n",
    "                    better = m1 if diff.mean() < 0 else m2\n",
    "                    print(f\"  Statistically significant difference - {better} is better\")\n",
    "                else:\n",
    "                    print(\"  No statistically significant difference\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a4290-8d0c-42dc-8226-bc5f54a24467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PERFORMANCE METRICS DEEP DIVE \n",
    "\n",
    "print(\"\\n4. ADVANCED PERFORMANCE METRICS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    # Get predictions from best model\n",
    "    y_pred = best_export_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    metrics = {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'R2': r2_score(y_test, y_pred),\n",
    "        'Max Error': max(abs(y_test - y_pred)),\n",
    "        'Mean Absolute Percentage Error': np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    }\n",
    "    \n",
    "    print(\"\\nComprehensive Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric == 'Max Error':\n",
    "            print(f\"  {metric:30}: ${value:,.2f}\")\n",
    "        elif 'Error' in metric:\n",
    "            print(f\"  {metric:30}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric:30}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate prediction intervals\n",
    "    residuals = y_test - y_pred\n",
    "    std_residual = np.std(residuals)\n",
    "    prediction_interval_95 = 1.96 * std_residual\n",
    "    \n",
    "    print(f\"\\nPrediction Intervals:\")\n",
    "    print(f\"  95% interval: ±${prediction_interval_95:,.2f}\")\n",
    "    print(f\"  Coverage (95%): {np.mean(np.abs(residuals) <= prediction_interval_95)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3594e-e951-47b7-b914-581b2cd5fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RESIDUAL DIAGNOSTICS \n",
    "\n",
    "print(\"\\n5. RESIDUAL ANALYSIS AND DIAGNOSTICS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    residuals = y_test - y_pred\n",
    "    standardized_residuals = (residuals - np.mean(residuals)) / np.std(residuals)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Residual Diagnostics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residual vs Predicted\n",
    "    axes[0, 0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Predicted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Predicted')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot of Residuals')\n",
    "    \n",
    "    # 3. Histogram of residuals\n",
    "    axes[0, 2].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 2].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Residuals')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Distribution of Residuals')\n",
    "    \n",
    "    # 4. Residuals vs Features (example)\n",
    "    if len(export_features) > 0:\n",
    "        feature_idx = 0\n",
    "        axes[1, 0].scatter(X_test.iloc[:, feature_idx], residuals, alpha=0.5)\n",
    "        axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[1, 0].set_xlabel(export_features[feature_idx])\n",
    "        axes[1, 0].set_ylabel('Residuals')\n",
    "        axes[1, 0].set_title(f'Residuals vs {export_features[feature_idx]}')\n",
    "    \n",
    "    # 5. Cook's distance (influence)\n",
    "    from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "    # Simplified calculation\n",
    "    n = len(residuals)\n",
    "    p = len(export_features)\n",
    "    hat_matrix = X_test_scaled @ np.linalg.pinv(X_test_scaled.T @ X_test_scaled) @ X_test_scaled.T\n",
    "    hii = np.diag(hat_matrix)\n",
    "    cooks_d = (residuals**2 / (p * np.mean(residuals**2))) * (hii / ((1 - hii)**2))\n",
    "    \n",
    "    axes[1, 1].stem(range(len(cooks_d)), cooks_d, linefmt='C0-', markerfmt='C0o', basefmt=\" \")\n",
    "    axes[1, 1].axhline(y=4/n, color='r', linestyle='--', label='4/n threshold')\n",
    "    axes[1, 1].set_xlabel('Observation Index')\n",
    "    axes[1, 1].set_ylabel(\"Cook's Distance\")\n",
    "    axes[1, 1].set_title(\"Cook's Distance for Influence\")\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Autocorrelation of residuals\n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    plot_acf(residuals, lags=20, ax=axes[1, 2])\n",
    "    axes[1, 2].set_title('Residual Autocorrelation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('residual_diagnostics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\nStatistical Tests for Residuals:\")\n",
    "    \n",
    "    # Normality test\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuals[:5000]) if len(residuals) > 5000 else stats.shapiro(residuals)\n",
    "    print(f\"Shapiro-Wilk normality test: p-value = {shapiro_p:.4f}\")\n",
    "    print(\"  Residuals are \" + (\"NOT \" if shapiro_p < 0.05 else \"\") + \"normally distributed\")\n",
    "    \n",
    "    # Homoscedasticity test (Breusch-Pagan)\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X_with_const = sm.add_constant(X_test_scaled)\n",
    "        bp_test = het_breuschpagan(residuals, X_with_const)\n",
    "        print(f\"\\nBreusch-Pagan test for heteroscedasticity:\")\n",
    "        print(f\"  LM statistic: {bp_test[0]:.4f}\")\n",
    "        print(f\"  p-value: {bp_test[1]:.4f}\")\n",
    "        print(\"  Residuals are \" + (\"heteroscedastic\" if bp_test[1] < 0.05 else \"homoscedastic\"))\n",
    "    except:\n",
    "        print(\"Could not perform Breusch-Pagan test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca8a20-8195-44ab-a61d-31a3a9e64c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. BIAS-VARIANCE ANALYSIS \n",
    "\n",
    "\n",
    "print(\"\\n6. BIAS-VARIANCE TRADEOFF ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "if len(X_train_sample) > 100 and y_train_sample is not None:\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        best_export_model, X_scaled_sample, y_train_sample,\n",
    "        cv=5, scoring='r2', n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Learning curve\n",
    "    axes[0].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\n",
    "    axes[0].plot(train_sizes, np.mean(val_scores, axis=1), 's-', label='Validation score')\n",
    "    axes[0].fill_between(train_sizes, \n",
    "                         np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),\n",
    "                         np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),\n",
    "                         alpha=0.1)\n",
    "    axes[0].fill_between(train_sizes,\n",
    "                         np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),\n",
    "                         np.mean(val_scores, axis=1) + np.std(val_scores, axis=1),\n",
    "                         alpha=0.1)\n",
    "    axes[0].set_xlabel('Training Set Size')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_title('Learning Curve')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bias-variance decomposition\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    gap = train_mean - val_mean\n",
    "    \n",
    "    axes[1].plot(train_sizes, train_mean, 'o-', label='Training Score (Variance)')\n",
    "    axes[1].plot(train_sizes, val_mean, 's-', label='Validation Score')\n",
    "    axes[1].plot(train_sizes, gap, '^-', label='Bias-Variance Gap')\n",
    "    axes[1].set_xlabel('Training Set Size')\n",
    "    axes[1].set_ylabel('R² Score')\n",
    "    axes[1].set_title('Bias-Variance Tradeoff')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bias_variance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBias-Variance Analysis:\")\n",
    "    print(f\"Final training score: {train_mean[-1]:.4f}\")\n",
    "    print(f\"Final validation score: {val_mean[-1]:.4f}\")\n",
    "    print(f\"Bias-Variance gap: {gap[-1]:.4f}\")\n",
    "    \n",
    "    if gap[-1] > 0.1:\n",
    "        print(\"Model shows signs of overfitting\")\n",
    "    elif gap[-1] < 0.05:\n",
    "        print(\"Model shows good generalization\")\n",
    "    else:\n",
    "        print(\"Model shows moderate bias-variance tradeoff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ccdf5-80fb-458b-869e-f1b75043a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. MODEL EXPLAINABILITY (SHAP)\n",
    "\n",
    "print(\"\\n7. MODEL EXPLAINABILITY ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "try:\n",
    "    print(\"Computing SHAP values for model interpretability...\")\n",
    "    \n",
    "    # Use a sample for SHAP computation\n",
    "    sample_idx = np.random.choice(len(X_test_scaled), size=min(100, len(X_test_scaled)), replace=False)\n",
    "    X_sample = X_test_scaled[sample_idx]\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_export_model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Create SHAP summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=export_features, show=False)\n",
    "    plt.title('SHAP Feature Importance Summary', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Individual prediction explanation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.decision_plot(explainer.expected_value, shap_values[0], \n",
    "                      feature_names=export_features, show=False)\n",
    "    plt.title('SHAP Decision Plot for Individual Prediction', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_individual.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate mean absolute SHAP values\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'Feature': export_features,\n",
    "        'SHAP_Importance': mean_abs_shap\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features by SHAP Importance:\")\n",
    "    print(shap_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save SHAP values\n",
    "    np.save('shap_values.npy', shap_values)\n",
    "    shap_importance.to_csv('shap_feature_importance.csv', index=False)\n",
    "    print(\"✓ SHAP analysis saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP computation failed: {e}\")\n",
    "    print(\"Using permutation importance instead...\")\n",
    "    \n",
    "    # Fallback to permutation importance\n",
    "    perm_importance = permutation_importance(\n",
    "        best_export_model, X_test_scaled, y_test,\n",
    "        n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    perm_df = pd.DataFrame({\n",
    "        'Feature': export_features,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features by Permutation Importance:\")\n",
    "    print(perm_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot permutation importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(perm_df['Feature'][:15], perm_df['Importance'][:15], \n",
    "             xerr=perm_df['Std'][:15])\n",
    "    plt.xlabel('Permutation Importance')\n",
    "    plt.title('Feature Importance (Permutation)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('permutation_importance.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa25f96-116c-46ec-bcb1-c56528755d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. PARTIAL DEPENDENCE PLOTS \n",
    "\n",
    "\n",
    "print(\"\\n8. PARTIAL DEPENDENCE ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "try:\n",
    "    # Select top features for PDP\n",
    "    top_features = shap_importance.head(4)['Feature'].tolist() if 'shap_importance' in locals() else export_features[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_features[:4]):\n",
    "        if feature in export_features:\n",
    "            feature_idx = export_features.index(feature)\n",
    "            \n",
    "            # Calculate partial dependence manually\n",
    "            unique_vals = np.unique(X_test_scaled[:, feature_idx])\n",
    "            if len(unique_vals) > 100:\n",
    "                unique_vals = np.percentile(X_test_scaled[:, feature_idx], np.linspace(0, 100, 50))\n",
    "            \n",
    "            pdp_vals = []\n",
    "            for val in unique_vals:\n",
    "                X_temp = X_test_scaled.copy()\n",
    "                X_temp[:, feature_idx] = val\n",
    "                preds = best_export_model.predict(X_temp)\n",
    "                pdp_vals.append(preds.mean())\n",
    "            \n",
    "            axes[idx].plot(unique_vals, pdp_vals, 'b-', linewidth=2)\n",
    "            axes[idx].set_xlabel(feature)\n",
    "            axes[idx].set_ylabel('Predicted Export')\n",
    "            axes[idx].set_title(f'Partial Dependence: {feature}')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Partial Dependence Plots for Top Features', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('partial_dependence.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Partial dependence plot failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75d678-8176-4aa0-bb22-4ace62651ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. MODEL ROBUSTNESS CHECKS \n",
    "\n",
    "\n",
    "print(\"\\n9. MODEL ROBUSTNESS ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"Performing robustness checks...\")\n",
    "\n",
    "robustness_tests = {}\n",
    "\n",
    "# 1. Out-of-sample performance by region\n",
    "if 'Region' in test_data.columns:\n",
    "    regions = test_data['Region'].unique()\n",
    "    regional_performance = {}\n",
    "    \n",
    "    for region in regions:\n",
    "        region_mask = test_data['Region'] == region\n",
    "        if region_mask.sum() > 5:\n",
    "            X_region = X_test_scaled[region_mask]\n",
    "            y_region = y_test[region_mask]\n",
    "            y_pred_region = best_export_model.predict(X_region)\n",
    "            regional_performance[region] = r2_score(y_region, y_pred_region)\n",
    "    \n",
    "    robustness_tests['regional_r2'] = regional_performance\n",
    "    \n",
    "    print(\"\\nRegional Performance (R²):\")\n",
    "    for region, r2 in regional_performance.items():\n",
    "        print(f\"  {region:25}: {r2:.4f}\")\n",
    "\n",
    "# 2. Temporal stability\n",
    "if 'Year_Value' in test_data.columns:\n",
    "    years = sorted(test_data['Year_Value'].unique())\n",
    "    yearly_performance = {}\n",
    "    \n",
    "    for year in years:\n",
    "        year_mask = test_data['Year_Value'] == year\n",
    "        if year_mask.sum() > 5:\n",
    "            X_year = X_test_scaled[year_mask]\n",
    "            y_year = y_test[year_mask]\n",
    "            y_pred_year = best_export_model.predict(X_year)\n",
    "            yearly_performance[year] = r2_score(y_year, y_pred_year)\n",
    "    \n",
    "    robustness_tests['yearly_r2'] = yearly_performance\n",
    "    \n",
    "    print(\"\\nYearly Performance (R²):\")\n",
    "    for year, r2 in yearly_performance.items():\n",
    "        print(f\"  {year}: {r2:.4f}\")\n",
    "\n",
    "# 3. Sensitivity to feature removal\n",
    "feature_sensitivity = {}\n",
    "for feature in export_features[:5]:  # Test with top 5 features\n",
    "    feature_idx = export_features.index(feature)\n",
    "    X_ablation = np.delete(X_test_scaled, feature_idx, axis=1)\n",
    "    \n",
    "    # Train a quick model without this feature\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model_ablation = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    \n",
    "    # Use corresponding training data\n",
    "    X_train_ablation = np.delete(X_scaled_sample, feature_idx, axis=1)\n",
    "    model_ablation.fit(X_train_ablation, y_train_sample)\n",
    "    \n",
    "    y_pred_ablation = model_ablation.predict(X_ablation)\n",
    "    r2_ablation = r2_score(y_test, y_pred_ablation)\n",
    "    \n",
    "    feature_sensitivity[feature] = metrics['R2'] - r2_ablation\n",
    "\n",
    "robustness_tests['feature_sensitivity'] = feature_sensitivity\n",
    "\n",
    "print(\"\\nFeature Sensitivity (R² drop when removed):\")\n",
    "for feature, sensitivity in sorted(feature_sensitivity.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feature:30}: {sensitivity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ca694-6289-447f-a705-e5e183a62af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053d51b-9536-4870-a003-517cecf4273d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41c886-2c60-4a96-87d3-cec0638f16a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da300527-d0f6-4212-a648-d38afa701749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95592f-f025-4721-b40a-2ecd55000995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5a1e5-6ef4-481b-b7f7-f09220ea9290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985ca93-bca2-4844-b54d-bcb152dec2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865ecab-c2ef-4940-bee4-0fb0541a2a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b5ca3-27cc-45a4-96f1-1e15a5f6e8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590c4f1-b325-4f81-a085-35985bde280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4daca0-4308-46df-841d-2b0a763c79f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657d123-4e3b-48d8-95ed-76db8f016ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f38b2-dcd5-4498-9080-3426b6ed23f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b16136-7806-4568-ae28-4ba6cdbd4336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245f587-49a6-4efb-8c8d-4b85a033a320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
