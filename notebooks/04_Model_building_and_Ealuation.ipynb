{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565c5c1e-8973-4045-83ec-cedd78710a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import shap\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9533ab-89cf-4628-b652-9dfaf36220df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED MODEL BUILDING AND EVALUATION\n",
      "================================================================================\n",
      "Loading pre-trained models and data...\n",
      "Error loading saved models: [Errno 2] No such file or directory: 'best_export_model.pkl'\n",
      "Please run Notebook 03 first to train models\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_export_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pre-trained models and data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     best_export_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_export_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     best_class_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_classification_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     export_scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_export_model.pkl'"
     ]
    }
   ],
   "source": [
    "# 1. SETUP AND DATA LOADING  \n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED MODEL BUILDING AND EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load pre-trained models and data\n",
    "print(\"Loading pre-trained models and data...\")\n",
    "\n",
    "try:\n",
    "    best_export_model = joblib.load('best_export_model.pkl')\n",
    "    best_class_model = joblib.load('best_classification_model.pkl')\n",
    "    export_scaler = joblib.load('export_scaler.pkl')\n",
    "    class_scaler = joblib.load('classification_scaler.pkl')\n",
    "    \n",
    "    # Load training data\n",
    "    df = pd.read_csv('world_trade_cleaned.csv')\n",
    "    country_df = df[df['Is_Country']].copy()\n",
    "    \n",
    "    # Load feature lists\n",
    "    with open('export_features.txt', 'r') as f:\n",
    "        export_features = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open('classification_features.txt', 'r') as f:\n",
    "        class_features = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(\"✓ Models and data loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading saved models: {e}\")\n",
    "    print(\"Please run Notebook 03 first to train models\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba6d58-36eb-40ea-9618-b69a7196b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HYPERPARAMETER OPTIMIZATION \n",
    "\n",
    "print(\"\\n2. HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "print(\"Performing advanced hyperparameter optimization...\")\n",
    "\n",
    "# Prepare data (using subset for faster optimization)\n",
    "X_train_export = pd.read_csv('world_trade_countries_only.csv')\n",
    "# Filter to get features needed for export prediction\n",
    "X_train_export = X_train_export[export_features].dropna()\n",
    "y_train_export = X_train_export['Export (US$ Thousand)_imputed'] if 'Export (US$ Thousand)_imputed' in X_train_export.columns else None\n",
    "\n",
    "if len(X_train_export) > 1000:\n",
    "    X_train_sample = X_train_export.sample(1000, random_state=42)\n",
    "    if y_train_export is not None:\n",
    "        y_train_sample = y_train_export.loc[X_train_sample.index]\n",
    "    else:\n",
    "        y_train_sample = None\n",
    "else:\n",
    "    X_train_sample = X_train_export\n",
    "    y_train_sample = y_train_export\n",
    "\n",
    "if y_train_sample is not None and len(X_train_sample) > 100:\n",
    "    # Scale features\n",
    "    X_scaled_sample = export_scaler.transform(X_train_sample)\n",
    "    \n",
    "    # Define Optuna objective function\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "        }\n",
    "        \n",
    "        model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Use cross-validation\n",
    "        scores = cross_val_score(model, X_scaled_sample, y_train_sample, \n",
    "                                cv=3, scoring='r2', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Running Bayesian optimization with Optuna...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters found:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best CV R²: {study.best_value:.4f}\")\n",
    "    \n",
    "    # Train optimized model\n",
    "    best_params = study.best_params\n",
    "    optimized_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "    optimized_model.fit(X_scaled_sample, y_train_sample)\n",
    "    \n",
    "    # Save optimized model\n",
    "    joblib.dump(optimized_model, 'optimized_export_model.pkl')\n",
    "    print(\"✓ Optimized model saved\")\n",
    "else:\n",
    "    print(\"Insufficient data for hyperparameter optimization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ba6d26-76c2-42ce-b42e-7875a990dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. MODEL COMPARISON WITH STATISTICAL TESTS\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_export_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load or create multiple model predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m models_to_compare \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbest_export_model\u001b[49m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_export_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(best_export_model)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: XGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare test data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_data \u001b[38;5;241m=\u001b[39m country_df[export_features \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExport (US$ Thousand)_imputed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear_Value\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_export_model' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. MODEL COMPARISON WITH STATISTICAL TESTS \n",
    "\n",
    "\n",
    "print(\"\\n3. MODEL COMPARISON WITH STATISTICAL TESTS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Load or create multiple model predictions\n",
    "models_to_compare = {\n",
    "    'Random Forest': best_export_model,\n",
    "    'Linear Regression': joblib.load('best_export_model.pkl') if 'linear' in str(best_export_model).lower() else None,\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Prepare test data\n",
    "test_data = country_df[export_features + ['Export (US$ Thousand)_imputed', 'Year_Value']].dropna()\n",
    "test_data = test_data[test_data['Year_Value'] == test_data['Year_Value'].max()]\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    X_test = test_data[export_features]\n",
    "    y_test = test_data['Export (US$ Thousand)_imputed']\n",
    "    X_test_scaled = export_scaler.transform(X_test)\n",
    "    \n",
    "    # Collect predictions\n",
    "    predictions = {}\n",
    "    for name, model in models_to_compare.items():\n",
    "        if model is not None:\n",
    "            try:\n",
    "                if name != 'Random Forest':\n",
    "                    model.fit(X_test_scaled[:100], y_test[:100])  # Quick fit\n",
    "                preds = model.predict(X_test_scaled)\n",
    "                predictions[name] = preds\n",
    "                print(f\"{name:20} R²: {r2_score(y_test, preds):.4f}\")\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Perform Diebold-Mariano test for model comparison\n",
    "    if len(predictments) >= 2:\n",
    "        print(\"\\nPerforming model comparison tests...\")\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = {}\n",
    "        for name, preds in predictions.items():\n",
    "            errors[name] = y_test - preds\n",
    "        \n",
    "        # Compare each pair\n",
    "        model_names = list(errors.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                m1, m2 = model_names[i], model_names[j]\n",
    "                \n",
    "                # Calculate difference in squared errors\n",
    "                diff = errors[m1]**2 - errors[m2]**2\n",
    "                \n",
    "                # Perform t-test\n",
    "                t_stat, p_value = stats.ttest_1samp(diff, 0)\n",
    "                \n",
    "                print(f\"\\n{m1} vs {m2}:\")\n",
    "                print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "                print(f\"  p-value: {p_value:.4f}\")\n",
    "                if p_value < 0.05:\n",
    "                    better = m1 if diff.mean() < 0 else m2\n",
    "                    print(f\"  Statistically significant difference - {better} is better\")\n",
    "                else:\n",
    "                    print(\"  No statistically significant difference\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a4290-8d0c-42dc-8226-bc5f54a24467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3594e-e951-47b7-b914-581b2cd5fe59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca8a20-8195-44ab-a61d-31a3a9e64c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ccdf5-80fb-458b-869e-f1b75043a138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa25f96-116c-46ec-bcb1-c56528755d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75d678-8176-4aa0-bb22-4ace62651ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ca694-6289-447f-a705-e5e183a62af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053d51b-9536-4870-a003-517cecf4273d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41c886-2c60-4a96-87d3-cec0638f16a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da300527-d0f6-4212-a648-d38afa701749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95592f-f025-4721-b40a-2ecd55000995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5a1e5-6ef4-481b-b7f7-f09220ea9290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985ca93-bca2-4844-b54d-bcb152dec2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865ecab-c2ef-4940-bee4-0fb0541a2a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b5ca3-27cc-45a4-96f1-1e15a5f6e8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590c4f1-b325-4f81-a085-35985bde280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4daca0-4308-46df-841d-2b0a763c79f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657d123-4e3b-48d8-95ed-76db8f016ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f38b2-dcd5-4498-9080-3426b6ed23f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b16136-7806-4568-ae28-4ba6cdbd4336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245f587-49a6-4efb-8c8d-4b85a033a320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
