{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ce488c-5d3b-462e-a138-1777d1074e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc6d7ba-be88-4b42-a7fd-69deaccaf78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WORLD TRADE PREDICTIVE MODELING\n",
      "================================================================================\n",
      "Dataset loaded: 7695 rows, 42 columns\n",
      "Years available: [np.int64(1988), np.int64(1989), np.int64(1990), np.int64(1991), np.int64(1992), np.int64(1993), np.int64(1994), np.int64(1995), np.int64(1996), np.int64(1997), np.int64(1998), np.int64(1999), np.int64(2000), np.int64(2001), np.int64(2002), np.int64(2003), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021)]\n"
     ]
    }
   ],
   "source": [
    "# 1. DATA PREPARATION AND PROBLEM FORMULATION \n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WORLD TRADE PREDICTIVE MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv('../notebooks/world_trade_cleaned.csv')\n",
    "country_df = df[df['Is_Country']].copy()\n",
    "\n",
    "print(f\"Dataset loaded: {country_df.shape[0]} rows, {country_df.shape[1]} columns\")\n",
    "print(f\"Years available: {sorted(country_df['Year_Value'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9954d660-ebca-429b-b810-4731f5902345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. PROBLEM FORMULATION\n",
      "----------------------------------------\n",
      "Three modeling approaches will be implemented:\n",
      "1. Export Value Prediction (Regression)\n",
      "2. Trade Balance Direction Prediction (Classification)\n",
      "3. Trade Growth Rate Prediction (Regression)\n"
     ]
    }
   ],
   "source": [
    "# 2. PROBLEM FORMULATION \n",
    "\n",
    "print(\"\\n2. PROBLEM FORMULATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# We'll create three predictive problems:\n",
    "# 1. Regression: Predict export value\n",
    "# 2. Classification: Predict trade balance direction (surplus/deficit)\n",
    "# 3. Time Series: Predict next year's trade metrics\n",
    "\n",
    "print(\"Three modeling approaches will be implemented:\")\n",
    "print(\"1. Export Value Prediction (Regression)\")\n",
    "print(\"2. Trade Balance Direction Prediction (Classification)\")\n",
    "print(\"3. Trade Growth Rate Prediction (Regression)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81490bc6-7512-42cb-a1ca-b563cebfafe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. FEATURE ENGINEERING\n",
      "----------------------------------------\n",
      "Total features after engineering: 66\n",
      "Sample engineered features: ['Export_3yr_avg', 'Import_3yr_avg', 'Export_Import_Ratio', 'Tariff_Differential', 'Region_Export_Mean', 'Region_Export_Std', 'Region_Import_Mean', 'Region_Import_Std', 'Export_Relative_to_Region', 'Import_Relative_to_Region']\n"
     ]
    }
   ],
   "source": [
    "# 3. FEATURE ENGINEERING \n",
    "\n",
    "print(\"\\n3. FEATURE ENGINEERING\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Create lag features for time series analysis\n",
    "country_df = country_df.sort_values(['Partner Name', 'Year_Value'])\n",
    "\n",
    "# Create lag features (previous year's values)\n",
    "lag_features = [\n",
    "    'Export (US$ Thousand)_imputed',\n",
    "    'Import (US$ Thousand)_imputed',\n",
    "    'Trade_Balance',\n",
    "    'Total_Trade',\n",
    "    'AHS Simple Average (%)',\n",
    "    'MFN Simple Average (%)'\n",
    "]\n",
    "\n",
    "for feature in lag_features:\n",
    "    if feature in country_df.columns:\n",
    "        country_df[f'{feature}_lag1'] = country_df.groupby('Partner Name')[feature].shift(1)\n",
    "        country_df[f'{feature}_lag2'] = country_df.groupby('Partner Name')[feature].shift(2)\n",
    "\n",
    "# Create growth rate features\n",
    "country_df['Export_Growth_Rate'] = country_df.groupby('Partner Name')['Export (US$ Thousand)_imputed'].pct_change()\n",
    "country_df['Import_Growth_Rate'] = country_df.groupby('Partner Name')['Import (US$ Thousand)_imputed'].pct_change()\n",
    "\n",
    "# Create rolling statistics\n",
    "country_df['Export_3yr_avg'] = country_df.groupby('Partner Name')['Export (US$ Thousand)_imputed'].transform(\n",
    "    lambda x: x.rolling(3, min_periods=1).mean())\n",
    "country_df['Import_3yr_avg'] = country_df.groupby('Partner Name')['Import (US$ Thousand)_imputed'].transform(\n",
    "    lambda x: x.rolling(3, min_periods=1).mean())\n",
    "\n",
    "# Create interaction features\n",
    "country_df['Export_Import_Ratio'] = country_df['Export (US$ Thousand)_imputed'] / (country_df['Import (US$ Thousand)_imputed'] + 1)\n",
    "country_df['Tariff_Differential'] = country_df['AHS Simple Average (%)'] - country_df['MFN Simple Average (%)']\n",
    "\n",
    "# Create regional aggregates as features\n",
    "regional_stats = country_df.groupby(['Region', 'Year_Value']).agg({\n",
    "    'Export (US$ Thousand)_imputed': ['mean', 'std'],\n",
    "    'Import (US$ Thousand)_imputed': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "regional_stats.columns = ['Region', 'Year_Value', \n",
    "                         'Region_Export_Mean', 'Region_Export_Std',\n",
    "                         'Region_Import_Mean', 'Region_Import_Std']\n",
    "\n",
    "country_df = pd.merge(country_df, regional_stats, on=['Region', 'Year_Value'], how='left')\n",
    "\n",
    "# Normalize by regional statistics\n",
    "country_df['Export_Relative_to_Region'] = country_df['Export (US$ Thousand)_imputed'] / country_df['Region_Export_Mean']\n",
    "country_df['Import_Relative_to_Region'] = country_df['Import (US$ Thousand)_imputed'] / country_df['Region_Import_Mean']\n",
    "\n",
    "print(f\"Total features after engineering: {len(country_df.columns)}\")\n",
    "print(f\"Sample engineered features: {list(country_df.columns[-10:])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e371eb4-edd4-4e62-a1db-3414cfca5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. PROBLEM 1: EXPORT VALUE PREDICTION (REGRESSION)\n",
      "----------------------------------------\n",
      "Export prediction dataset: (7436, 14)\n",
      "Train size: (7200, 11), Test size: (236, 11)\n"
     ]
    }
   ],
   "source": [
    "# 4. PROBLEM 1: EXPORT VALUE PREDICTION \n",
    "\n",
    "print(\"\\n4. PROBLEM 1: EXPORT VALUE PREDICTION (REGRESSION)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Prepare dataset for export prediction\n",
    "export_features = [\n",
    "    # Lag features\n",
    "    'Export (US$ Thousand)_imputed_lag1',\n",
    "    'Import (US$ Thousand)_imputed_lag1',\n",
    "    'Trade_Balance_lag1',\n",
    "    \n",
    "    # Current year features (excluding export)\n",
    "    'Import (US$ Thousand)_imputed',\n",
    "    'AHS Simple Average (%)',\n",
    "    'MFN Simple Average (%)',\n",
    "    \n",
    "    # Engineered features\n",
    "    'Export_Import_Ratio',\n",
    "    'Tariff_Differential',\n",
    "    'Export_Relative_to_Region',\n",
    "    \n",
    "    # Regional features\n",
    "    'Region_Export_Mean',\n",
    "    'Region_Import_Mean'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'Export (US$ Thousand)_imputed'\n",
    "\n",
    "# Filter data with required features\n",
    "export_df = country_df[export_features + [target] + ['Year_Value', 'Partner Name']].copy()\n",
    "export_df = export_df.dropna()\n",
    "\n",
    "print(f\"Export prediction dataset: {export_df.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X_export = export_df[export_features]\n",
    "y_export = export_df[target]\n",
    "\n",
    "# Split data (by year for temporal validation)\n",
    "train_mask = export_df['Year_Value'] < export_df['Year_Value'].max()\n",
    "X_train_export = X_export[train_mask]\n",
    "X_test_export = X_export[~train_mask]\n",
    "y_train_export = y_export[train_mask]\n",
    "y_test_export = y_export[~train_mask]\n",
    "\n",
    "print(f\"Train size: {X_train_export.shape}, Test size: {X_test_export.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_export = StandardScaler()\n",
    "X_train_scaled_export = scaler_export.fit_transform(X_train_export)\n",
    "X_test_scaled_export = scaler_export.transform(X_test_export)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5db8ce-4ad4-4707-b072-7a8ddfe1f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. PROBLEM 2: TRADE BALANCE DIRECTION (CLASSIFICATION)\n",
      "----------------------------------------\n",
      "Classification dataset: (7432, 11)\n",
      "Class distribution:\n",
      "Trade_Surplus\n",
      "1    0.724031\n",
      "0    0.275969\n",
      "Name: proportion, dtype: float64\n",
      "Train size: (7196, 9), Test size: (236, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[0;32m     45\u001b[0m scaler_class \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 46\u001b[0m X_train_scaled_class \u001b[38;5;241m=\u001b[39m \u001b[43mscaler_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m X_test_scaled_class \u001b[38;5;241m=\u001b[39m scaler_class\u001b[38;5;241m.\u001b[39mtransform(X_test_class)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# 5. PROBLEM 2: TRADE BALANCE DIRECTION PREDICTION \n",
    "\n",
    "print(\"\\n5. PROBLEM 2: TRADE BALANCE DIRECTION (CLASSIFICATION)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Create binary target: 1 for trade surplus, 0 for deficit\n",
    "country_df['Trade_Surplus'] = (country_df['Trade_Balance'] > 0).astype(int)\n",
    "\n",
    "# Features for classification\n",
    "class_features = [\n",
    "    'Export (US$ Thousand)_imputed_lag1',\n",
    "    'Import (US$ Thousand)_imputed_lag1',\n",
    "    'Export_Growth_Rate',\n",
    "    'Import_Growth_Rate',\n",
    "    'Export_Import_Ratio',\n",
    "    'AHS Simple Average (%)',\n",
    "    'MFN Simple Average (%)',\n",
    "    'Region_Export_Mean',\n",
    "    'Region_Import_Mean'\n",
    "]\n",
    "\n",
    "target_class = 'Trade_Surplus'\n",
    "\n",
    "# Prepare classification dataset\n",
    "class_df = country_df[class_features + [target_class] + ['Year_Value']].copy()\n",
    "class_df = class_df.dropna()\n",
    "\n",
    "print(f\"Classification dataset: {class_df.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(class_df[target_class].value_counts(normalize=True))\n",
    "\n",
    "X_class = class_df[class_features]\n",
    "y_class = class_df[target_class]\n",
    "\n",
    "# Split data\n",
    "train_mask_class = class_df['Year_Value'] < class_df['Year_Value'].max()\n",
    "X_train_class = X_class[train_mask_class]\n",
    "X_test_class = X_class[~train_mask_class]\n",
    "y_train_class = y_class[train_mask_class]\n",
    "y_test_class = y_class[~train_mask_class]\n",
    "\n",
    "print(f\"Train size: {X_train_class.shape}, Test size: {X_test_class.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_class = StandardScaler()\n",
    "X_train_scaled_class = scaler_class.fit_transform(X_train_class)\n",
    "X_test_scaled_class = scaler_class.transform(X_test_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ed5bb-0a5a-4b3d-a0aa-e89df16cc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PROBLEM 3: TRADE GROWTH RATE PREDICTION \n",
    "\n",
    "print(\"\\n6. PROBLEM 3: TRADE GROWTH RATE PREDICTION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Features for growth rate prediction\n",
    "growth_features = [\n",
    "    'Export (US$ Thousand)_imputed_lag1',\n",
    "    'Import (US$ Thousand)_imputed_lag1',\n",
    "    'Export_Growth_Rate',\n",
    "    'Export_3yr_avg',\n",
    "    'AHS Simple Average (%)',\n",
    "    'MFN Simple Average (%)',\n",
    "    'Export_Relative_to_Region',\n",
    "    'Region_Export_Mean'\n",
    "]\n",
    "\n",
    "target_growth = 'Export_Growth_Rate'\n",
    "\n",
    "# Prepare growth dataset\n",
    "growth_df = country_df[growth_features + [target_growth] + ['Year_Value']].copy()\n",
    "growth_df = growth_df.dropna()\n",
    "\n",
    "print(f\"Growth prediction dataset: {growth_df.shape}\")\n",
    "\n",
    "X_growth = growth_df[growth_features]\n",
    "y_growth = growth_df[target_growth]\n",
    "\n",
    "# Split data\n",
    "train_mask_growth = growth_df['Year_Value'] < growth_df['Year_Value'].max()\n",
    "X_train_growth = X_growth[train_mask_growth]\n",
    "X_test_growth = X_growth[~train_mask_growth]\n",
    "y_train_growth = y_growth[train_mask_growth]\n",
    "y_test_growth = y_growth[~train_mask_growth]\n",
    "\n",
    "print(f\"Train size: {X_train_growth.shape}, Test size: {X_test_growth.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_growth = StandardScaler()\n",
    "X_train_scaled_growth = scaler_growth.fit_transform(X_train_growth)\n",
    "X_test_scaled_growth = scaler_growth.transform(X_test_growth)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffc10c-01ab-4a5f-9bfb-fb6d09a5ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. FEATURE SELECTION \n",
    "\n",
    "\n",
    "print(\"\\n7. FEATURE SELECTION ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "print(\"\\nExport Prediction - Feature Importance:\")\n",
    "\n",
    "rf_importance = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_importance.fit(X_train_scaled_export, y_train_export)\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': export_features,\n",
    "    'Importance': rf_importance.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(importances.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importances['Feature'][:15], importances['Importance'][:15])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features for Export Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Use SelectKBest for feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_selected = selector.fit_transform(X_train_scaled_export, y_train_export)\n",
    "\n",
    "selected_features = [export_features[i] for i in selector.get_support(indices=True)]\n",
    "print(f\"\\nSelected features (KBest): {selected_features}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed902e15-b842-4745-a2ba-12e5878bb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. BASELINE MODELS\n",
    "\n",
    "\n",
    "print(\"\\n8. BASELINE MODEL TRAINING\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Define baseline models for regression\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.01),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate regression models\n",
    "print(\"\\nExport Prediction Model Performance:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "regression_results = []\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled_export, y_train_export)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled_export)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test_export, y_pred)\n",
    "    mse = mean_squared_error(y_test_export, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_export, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled_export, y_train_export, \n",
    "                               cv=5, scoring='r2')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    regression_results.append({\n",
    "        'Model': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'CV_R2_Mean': cv_mean,\n",
    "        'CV_R2_Std': cv_std\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20} R²: {r2:.4f} | RMSE: {rmse:,.0f} | CV R²: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\n",
    "regression_df = pd.DataFrame(regression_results).sort_values('R2', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f86b7-9761-4a78-921a-fc38996e2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. CLASSIFICATION MODELS \n",
    "\n",
    "\n",
    "print(\"\\n9. CLASSIFICATION MODEL TRAINING\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nTrade Balance Classification Performance:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "classification_results = []\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled_class, y_train_class)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled_class)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled_class)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_class, y_pred)\n",
    "    precision = precision_score(y_test_class, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_class, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_class, y_pred, average='weighted')\n",
    "    \n",
    "    auc = roc_auc_score(y_test_class, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled_class, y_train_class, \n",
    "                               cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    classification_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUC': auc,\n",
    "        'CV_Accuracy_Mean': cv_mean,\n",
    "        'CV_Accuracy_Std': cv_std\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20} Accuracy: {accuracy:.4f} | F1: {f1:.4f} | AUC: {auc if auc else 'N/A':.4f}\")\n",
    "\n",
    "classification_df = pd.DataFrame(classification_results).sort_values('Accuracy', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b07f2c-b2b1-4470-9c40-5bceb0190872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. GROWTH RATE PREDICTION MODELS \n",
    "\n",
    "print(\"\\n10. GROWTH RATE PREDICTION MODELS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "growth_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nGrowth Rate Prediction Performance:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "growth_results = []\n",
    "\n",
    "for name, model in growth_models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled_growth, y_train_growth)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled_growth)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test_growth, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_growth, y_pred))\n",
    "    r2 = r2_score(y_test_growth, y_pred)\n",
    "    \n",
    "    growth_results.append({\n",
    "        'Model': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20} R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "growth_df = pd.DataFrame(growth_results).sort_values('R2', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3743674-7ff4-4fe4-959f-0641e13947ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. MODEL COMPARISON AND VISUALIZATION \n",
    "\n",
    "print(\"\\n11. MODEL COMPARISON\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Regression models comparison\n",
    "reg_plot = regression_df.plot(kind='bar', x='Model', y=['R2', 'CV_R2_Mean'], \n",
    "                             ax=axes[0, 0], color=['blue', 'orange'])\n",
    "axes[0, 0].set_title('Export Prediction: R² Comparison')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].legend(['Test R²', 'CV R²'])\n",
    "\n",
    "# Classification accuracy\n",
    "class_plot = classification_df.plot(kind='bar', x='Model', y=['Accuracy', 'CV_Accuracy_Mean'],\n",
    "                                   ax=axes[0, 1], color=['green', 'purple'])\n",
    "axes[0, 1].set_title('Trade Balance Classification: Accuracy Comparison')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].legend(['Test Accuracy', 'CV Accuracy'])\n",
    "\n",
    "# Error metrics for regression\n",
    "axes[1, 0].bar(regression_df['Model'], regression_df['RMSE'], color='red', alpha=0.6)\n",
    "axes[1, 0].set_title('Export Prediction: RMSE by Model')\n",
    "axes[1, 0].set_ylabel('RMSE (Lower is better)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature importance from best model\n",
    "best_reg_model_name = regression_df.iloc[0]['Model']\n",
    "best_reg_model = regression_models[best_reg_model_name]\n",
    "if hasattr(best_reg_model, 'feature_importances_'):\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Feature': export_features,\n",
    "        'Importance': best_reg_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    axes[1, 1].barh(feature_imp['Feature'], feature_imp['Importance'])\n",
    "    axes[1, 1].set_title(f'Feature Importance: {best_reg_model_name}')\n",
    "    axes[1, 1].set_xlabel('Importance')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e0332-c848-435c-8c0d-6fae8e072b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. MODEL PERSISTENCE \n",
    "\n",
    "print(\"\\n12. MODEL PERSISTENCE\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Save the best models\n",
    "best_export_model = regression_models[regression_df.iloc[0]['Model']]\n",
    "best_class_model = classification_models[classification_df.iloc[0]['Model']]\n",
    "\n",
    "# Retrain on full training data\n",
    "best_export_model.fit(X_train_scaled_export, y_train_export)\n",
    "best_class_model.fit(X_train_scaled_class, y_train_class)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(best_export_model, 'best_export_model.pkl')\n",
    "joblib.dump(best_class_model, 'best_classification_model.pkl')\n",
    "joblib.dump(scaler_export, 'export_scaler.pkl')\n",
    "joblib.dump(scaler_class, 'classification_scaler.pkl')\n",
    "\n",
    "print(\"Models and scalers saved successfully:\")\n",
    "print(\"1. best_export_model.pkl - Best export prediction model\")\n",
    "print(\"2. best_classification_model.pkl - Best trade balance classification model\")\n",
    "print(\"3. export_scaler.pkl - Feature scaler for export model\")\n",
    "print(\"4. classification_scaler.pkl - Feature scaler for classification model\")\n",
    "\n",
    "# Save feature lists\n",
    "with open('export_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(export_features))\n",
    "\n",
    "with open('classification_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(class_features))\n",
    "\n",
    "print(\"Feature lists saved to text files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a10adc-2ff4-40cc-9dea-2c255f007f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. PREDICTION DEMONSTRATION\n",
    "\n",
    "\n",
    "print(\"\\n13. PREDICTION DEMONSTRATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Make sample predictions\n",
    "sample_idx = 0\n",
    "if len(X_test_scaled_export) > 0:\n",
    "    sample_prediction = best_export_model.predict(X_test_scaled_export[sample_idx:sample_idx+1])\n",
    "    actual_value = y_test_export.iloc[sample_idx]\n",
    "    \n",
    "    print(f\"\\nSample Export Prediction:\")\n",
    "    print(f\"Predicted: ${sample_prediction[0]:,.2f}\")\n",
    "    print(f\"Actual: ${actual_value:,.2f}\")\n",
    "    print(f\"Error: ${abs(sample_prediction[0] - actual_value):,.2f}\")\n",
    "    print(f\"Percentage Error: {abs((sample_prediction[0] - actual_value)/actual_value)*100:.2f}%\")\n",
    "\n",
    "# Classification prediction\n",
    "if len(X_test_scaled_class) > 0:\n",
    "    sample_class_pred = best_class_model.predict(X_test_scaled_class[sample_idx:sample_idx+1])\n",
    "    sample_class_proba = best_class_model.predict_proba(X_test_scaled_class[sample_idx:sample_idx+1])\n",
    "    \n",
    "    print(f\"\\nSample Trade Balance Classification:\")\n",
    "    print(f\"Predicted: {'Surplus' if sample_class_pred[0] == 1 else 'Deficit'}\")\n",
    "    print(f\"Confidence: {max(sample_class_proba[0]):.2%}\")\n",
    "    print(f\"Actual: {'Surplus' if y_test_class.iloc[sample_idx] == 1 else 'Deficit'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576626e1-eb76-45fb-a383-d3096723283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. TRAINING SUMMARY\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n14. TRAINING SUMMARY\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(f\"\\nBest Export Prediction Model: {regression_df.iloc[0]['Model']}\")\n",
    "print(f\"  R² Score: {regression_df.iloc[0]['R2']:.4f}\")\n",
    "print(f\"  RMSE: {regression_df.iloc[0]['RMSE']:,.2f}\")\n",
    "print(f\"  Cross-Validation R²: {regression_df.iloc[0]['CV_R2_Mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Classification Model: {classification_df.iloc[0]['Model']}\")\n",
    "print(f\"  Accuracy: {classification_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {classification_df.iloc[0]['F1']:.4f}\")\n",
    "print(f\"  AUC: {classification_df.iloc[0]['AUC'] if classification_df.iloc[0]['AUC'] else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"  Total training samples: {len(X_train_export)}\")\n",
    "print(f\"  Total test samples: {len(X_test_export)}\")\n",
    "print(f\"  Feature count: {len(export_features)}\")\n",
    "\n",
    "# Save training summary\n",
    "training_summary = {\n",
    "    'export_model': regression_df.iloc[0].to_dict(),\n",
    "    'classification_model': classification_df.iloc[0].to_dict(),\n",
    "    'training_stats': {\n",
    "        'train_samples': len(X_train_export),\n",
    "        'test_samples': len(X_test_export),\n",
    "        'features_count': len(export_features),\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(training_summary, f, indent=4)\n",
    "\n",
    "print(\"\\nTraining summary saved to 'training_summary.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8af95-03dd-4f64-bacd-1fb36d8db79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd92a0f-bcc0-4d30-8a84-94e0e08c7c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4255d36-66a2-4a67-9c5e-5f1b17a2b4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735f1dd-1d9a-4b2a-bc5b-91e67be4e93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285bd31-5371-45e7-b6bc-b5493beb9750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4a6fb-5b7c-41e2-88ab-5261d1efb9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73c48a-1576-48a2-858d-876a2e97877e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5f30f-47b3-4067-a89d-731009182629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635c1d6-0c88-4d64-91ce-cfed11181052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
